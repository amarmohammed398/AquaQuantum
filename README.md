# AquaQuantum ğŸŸğŸ”¬

**AquaQuantum** is a hybrid AI-powered mobile app for identifying fish species using a Convolutional Neural Network (CNN) enhanced with a Quantum Neural Network (QNN). It provides real-time classification, geotagging, and scan history via a user-friendly mobile interface powered by React Native and Expo.

> ğŸ’¡ Built with PyTorch, PennyLane, FastAPI, and Expo (React Native)

---

## ğŸ“± Features

- ğŸ“· **Camera-based Capture**: Use your device camera to capture fish images.
- âš›ï¸ **Quantum Hybrid Classifier**: Combines EfficientNet-B0 CNN with a 4-qubit Quantum Neural Network.
- ğŸ—ºï¸ **Geotagging**: Saves location data of each scan if permission is granted.
- ğŸ“– **Scan History**: Stores all classified images with metadata.
- ğŸ§­ **Map Visualization**: Visualize previous scans pinned on a map.
- ğŸ“Š **Statistics Dashboard**: Displays total scans and unique species count.
- ğŸ“˜ **Interactive Help Guide**: Easily understand app functionality.
- ğŸ”— **Backend API**: Built with FastAPI to serve a quantum-enhanced classification model.
- ğŸ§ª **Supports Offline History Storage**: Uses local device storage.

---

## ğŸ› ï¸ Installation & Setup

### ğŸ“¦ Backend (Quantum Hybrid Model Server)

1. Install dependencies:
```bash
pip install torch torchvision torchaudio
pip install pennylane pennylane-lightning
pip install matplotlib scikit-learn tqdm fastapi uvicorn
```
2. Run the server:
```bash
uvicorn server:app --reload --host 0.0.0.0 --port 8000
```
Ensure the model weights (hybrid_qcnn_model_v1.0.pth) and species_mapping.json are in the same directory.

## ğŸ“± Mobile App (Expo/React Native)
1. Install dependencies:

```bash
npm install
```
2. Start the app:
```bash
npx expo start
```
3. Required Configuration:
 
   - Update the IP under API_URLS.dev to match your local network server IP.

4. Permissions Required:

   - Camera access

   - Location access (optional but enhances experience)

## ğŸš€ How to Use
1. Launch the App â€“ The splash screen leads to the Home.

2. Start a Scan â€“ Navigate to the camera, capture an image.

3. Receive Analysis â€“ Image is classified by the backend model and metadata is recorded.

4. View Results:

- ğŸ—‚ï¸ History Tab: Review your past scans

- ğŸ—ºï¸ Map Tab: Explore geolocated results

5. Use Help Tab â€“ Get user guidance at any time.

## ğŸ“ˆ Planned Improvements
- âœ… Replace dummy classification on frontend with live API prediction.

- ğŸ§  Improve QNN architecture and extend quantum layers.

- â˜ï¸ Deploy FastAPI on cloud or use tunneling for public access.

- ğŸ“² Expand iOS support and responsive design.

- ğŸ“¤ Add user account integration for sync and backup.

- ğŸ” Add filtering options in history view.

## ğŸ‘¨â€ğŸ“ Academic Notice
This project is developed as part of a final year Synoptic Project. It explores quantum-enhanced machine learning using PennyLane and PyTorch and serves as a demonstrator for hybrid quantum-classical pipelines applied in real-world mobile environments.

## ğŸ‘¨â€ğŸ’» Author
**Name:** Amar Mukhtar Mohammed

**Institution:** The Manchester Metropolitan University

**Course:** B.Sc Computer Science

**Supervisors:** Dr. Kate MacFarlane & Dr. Matthew Shardlow

## ğŸ“‚ Project Directory Structure

```plaintext  
â”œâ”€â”€ Home Server/  
â”‚   â”œâ”€â”€ server.py               â†’ FastAPI server for classification  
â”‚   â”œâ”€â”€ hybrid_qcnn_model_v1.0.pth  â†’ Trained PyTorch model  
â”‚   â”œâ”€â”€ species_mapping.json    â†’ ID to species name map  

â”œâ”€â”€ Expo React Native App/  
â”‚   â”œâ”€â”€ AppNavigator.js         â†’ App routing and screens  
â”‚   â”œâ”€â”€ CameraScreen.js         â†’ Main scan/capture logic  
â”‚   â”œâ”€â”€ HistoryScreen.js        â†’ Saved results viewer  
â”‚   â”œâ”€â”€ MapScreen.js            â†’ Geotag visualization  
â”‚   â”œâ”€â”€ HelpScreen.js           â†’ Help and usage instructions  
â”‚   â”œâ”€â”€ api.js                  â†’ Image upload and classification request  
â”‚   â”œâ”€â”€ HomeScreen.js           â†’ Dashboard with stats and navigation  
â”‚   â”œâ”€â”€ SplashScreen.js         â†’ Intro loading screen  

```

## ğŸ§ª Example API Usage
- Endpoint: POST /predict/
- Payload: JPEG image
Response:

```json
{
  "class_id": 12,
  "class_name": "Rainbow Trout",
  "confidence": 98.73
}
```

## ğŸ§  Tech Stack
- Frontend: React Native + Expo

- Backend: FastAPI, PyTorch, PennyLane

- Quantum Engine: PennyLane Lightning

- Model: EfficientNet-B0 + BasicEntanglerLayers

- Storage: AsyncStorage (local)

- Map: react-native-maps

## ğŸ“¸ Screenshots

> Disclaimer: The data shown in the screenshots below is dummy data and used for demonstration purposes only. 

### ğŸ” Home View 
<img src="screenshots/Screenshot7.png" width="300"/>  
<img src="screenshots/Screenshot2.png" width="300"/>

### ğŸ§¾ Camera View
<img src="screenshots/Screenshot3.png" width="300"/>  
<img src="screenshots/Screenshot4.png" width="300"/>

### ğŸ—ºï¸ Map View
<img src="screenshots/Screenshot8.jpg" width="300"/>

### ğŸ§¾ History Log
<img src="screenshots/Screenshot5.png" width="300"/>


## ğŸ“œ License
This is a student research project. Not for commercial use. The project was developed in full compliance with MMU academic integrity policies,




