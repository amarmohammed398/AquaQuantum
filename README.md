# AquaQuantum ğŸŸğŸ”¬

**AquaQuantum** is a hybrid AI-powered mobile app for identifying fish species using a Convolutional Neural Network (CNN) enhanced with a Quantum Neural Network (QNN). It provides real-time classification, geotagging, and scan history via a user-friendly mobile interface powered by React Native and Expo.

> ğŸ’¡ Built with PyTorch, PennyLane, FastAPI, and Expo (React Native)

---

## ğŸ“± Features

- ğŸ“· **Camera-based Capture**: Use your device camera to capture fish images.
- âš›ï¸ **Quantum Hybrid Classifier**: Combines EfficientNet-B0 CNN with a 4-qubit Quantum Neural Network.
- ğŸ—ºï¸ **Geotagging**: Saves location data of each scan if permission is granted.
- ğŸ“– **Scan History**: Stores all classified images with metadata.
- ğŸ§­ **Map Visualization**: Visualize previous scans pinned on a map.
- ğŸ“Š **Statistics Dashboard**: Displays total scans and unique species count.
- ğŸ“˜ **Interactive Help Guide**: Easily understand app functionality.
- ğŸ”— **Backend API**: Built with FastAPI to serve a quantum-enhanced classification model.
- ğŸ§ª **Supports Offline History Storage**: Uses local device storage.

---

## ğŸ› ï¸ Installation & Setup

### ğŸ“¦ Backend (Quantum Hybrid Model Server)

1. Install dependencies:
```bash
pip install torch torchvision torchaudio
pip install pennylane pennylane-lightning
pip install matplotlib scikit-learn tqdm fastapi uvicorn
```
2. Run the server:
```bash
uvicorn server:app --reload --host 0.0.0.0 --port 8000
```
Ensure the model weights (hybrid_qcnn_model_v1.0.pth) and species_mapping.json are in the same directory.

## ğŸ“± Mobile App (Expo/React Native)
1. Install dependencies:

```bash
npm install
```
2. Start the app:
```bash
npx expo start
```
3. Required Configuration:
 
   - Update the IP under API_URLS.dev to match your local network server IP.

4. Permissions Required:

   - Camera access

   - Location access (optional but enhances experience)

## ğŸš€ How to Use
1. Launch the App â€“ The splash screen leads to the Home.

2. Start a Scan â€“ Navigate to the camera, capture an image.

3. Receive Analysis â€“ Image is classified by the backend model and metadata is recorded.

4. View Results:

- ğŸ—‚ï¸ History Tab: Review your past scans

- ğŸ—ºï¸ Map Tab: Explore geolocated results

5. Use Help Tab â€“ Get user guidance at any time.

## ğŸ“ˆ Planned Improvements
- âœ… Replace dummy classification on frontend with live API prediction.

- ğŸ§  Improve QNN architecture and extend quantum layers.

- â˜ï¸ Deploy FastAPI on cloud or use tunneling for public access.

- ğŸ“² Expand iOS support and responsive design.

- ğŸ“¤ Add user account integration for sync and backup.

- ğŸ” Add filtering options in history view.

## ğŸ‘¨â€ğŸ“ Academic Notice
This project is developed as part of a final year Synoptic Project. It explores quantum-enhanced machine learning using PennyLane and PyTorch and serves as a demonstrator for hybrid quantum-classical pipelines applied in real-world mobile environments.

## ğŸ‘¨â€ğŸ’» Author
**Name:** Amar Mukhtar Mohammed

**Institution:** The Manchester Metropolitan University

**Course:** B.Sc Computer Science

**Supervisors:** Dr. Kate MacFarlane & Dr. Matthew Shardlow

## ğŸ“‚ Project Directory Structure

```plaintext  
â”œâ”€â”€ Home Server/  
â”‚   â”œâ”€â”€ server.py               â†’ FastAPI server for classification  
â”‚   â”œâ”€â”€ hybrid_qcnn_model_v1.0.pth  â†’ Trained PyTorch model  
â”‚   â”œâ”€â”€ species_mapping.json    â†’ ID to species name map  

â”œâ”€â”€ Expo React Native App/  
â”‚   â”œâ”€â”€ AppNavigator.js         â†’ App routing and screens  
â”‚   â”œâ”€â”€ CameraScreen.js         â†’ Main scan/capture logic  
â”‚   â”œâ”€â”€ HistoryScreen.js        â†’ Saved results viewer  
â”‚   â”œâ”€â”€ MapScreen.js            â†’ Geotag visualization  
â”‚   â”œâ”€â”€ HelpScreen.js           â†’ Help and usage instructions  
â”‚   â”œâ”€â”€ api.js                  â†’ Image upload and classification request  
â”‚   â”œâ”€â”€ HomeScreen.js           â†’ Dashboard with stats and navigation  
â”‚   â”œâ”€â”€ SplashScreen.js         â†’ Intro loading screen  

```

## ğŸ§ª Example API Usage
- Endpoint: POST /predict/
- Payload: JPEG image
Response:

```json
{
  "class_id": 12,
  "class_name": "Rainbow Trout",
  "confidence": 98.73
}
```

## ğŸ§  Tech Stack
- Frontend: React Native + Expo

- Backend: FastAPI, PyTorch, PennyLane

- Quantum Engine: PennyLane Lightning

- Model: EfficientNet-B0 + BasicEntanglerLayers

- Storage: AsyncStorage (local)

- Map: react-native-maps

## ğŸ“¸ Screenshots

> Disclaimer: The data shown in the screenshots below is dummy data and used for demonstration purposes only. 

### ğŸ” Home View 
<img src="screenshots/Screenshot7.png" width="300"/>  
<img src="screenshots/Screenshot2.png" width="300"/>

### ğŸ§¾ Camera View
<img src="screenshots/Screenshot3.png" width="300"/>  
<img src="screenshots/Screenshot4.png" width="300"/>

### ğŸ—ºï¸ Map View
<img src="screenshots/Screenshot8.jpg" width="300"/>

### ğŸ§¾ History Log
<img src="screenshots/Screenshot5.png" width="300"/>


## ğŸ§¾ Development Environment Info

The following environment was used during the development of AquaQuantum:

```powershell
PS C:\Users\amarm\OneDrive - MMU\Year 3\Synoptic Project(1)\Expo React Native App\AquaQuantum> npx envinfo --system --binaries --npmPackages --npmGlobalPackages

System:
  OS: Windows 11 10.0.26100
  CPU: (16) x64 12th Gen Intel(R) Core(TM) i7-1270P
  Memory: 17.27 GB / 31.67 GB
Binaries:
  Node: 22.13.1 - C:\Program Files\nodejs\node.EXE
  npm: 11.1.0 - C:\Program Files\nodejs\npm.CMD
npmPackages:
  @babel/core: ^7.20.0 => 7.26.10 
  @expo/vector-icons: ^14.0.2 => 14.0.4 
  @react-native-async-storage/async-storage: ^2.1.2 => 2.1.2 
  @react-navigation/native: ^7.0.18 => 7.0.19 
  @react-navigation/native-stack: ^7.3.2 => 7.3.3 
  @types/react: ~18.3.12 => 18.3.20 
  expo: ~52.0.42 => 52.0.42
  expo-camera: ~16.0.18 => 16.0.18
  expo-image-picker: ~16.0.6 => 16.0.6
  expo-linear-gradient: ~14.0.2 => 14.0.2
  expo-location: ~18.0.10 => 18.0.10
  expo-media-library: ~17.0.6 => 17.0.6
  expo-status-bar: ~2.0.1 => 2.0.1
  lottie-ios: ^4.5.1 => 4.5.1
  lottie-react-native: 7.1.0 => 7.1.0
  react: 18.3.1 => 18.3.1
  react-native: ^0.76.8 => 0.76.8
  react-native-gesture-handler: ~2.20.2 => 2.20.2
  react-native-maps: 1.18.0 => 1.18.0
  react-native-reanimated: ~3.16.1 => 3.16.7
  react-native-safe-area-context: 4.12.0 => 4.12.0
  react-native-screens: ~4.4.0 => 4.4.0
  react-native-svg: 15.8.0 => 15.8.0
  typescript: ^5.3.3 => 5.8.2
npmGlobalPackages:
  expo-cli: 6.3.12
  npm: 11.1.0
```

## ğŸ“œ License
This is a student research project. Not for commercial use. The project was developed in full compliance with MMU academic integrity policies,




